{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94c15364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing .: 0it [00:00, ?it/s]\n",
      "Processing C081: 100%|██████████| 327/327 [00:07<00:00, 42.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images have been downsampled and saved to: 128x128_images\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Cell 2: Function Definition\n",
    "# Cell 2: Function Definition (recursive)\n",
    "def downsample_images(input_dir, output_dir, target_size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Downsamples all images in input_dir (e sue sottocartelle) a target_size\n",
    "    e ricrea la struttura di directory corrispondente in output_dir.\n",
    "    \"\"\"\n",
    "    # Crea la cartella di base di output\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Cammina tutta la gerarchia di input_dir\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        # Calcola percorso relativo rispetto a input_dir\n",
    "        rel_path = os.path.relpath(root, input_dir)\n",
    "        # Costruisci la cartella di destinazione corrispondente\n",
    "        target_dir = os.path.join(output_dir, rel_path)\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        \n",
    "        # Elabora tutti i file in questa cartella\n",
    "        for filename in tqdm(files, desc=f\"Processing {rel_path}\"):\n",
    "            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "                input_path = os.path.join(root, filename)\n",
    "                output_path = os.path.join(target_dir, filename)\n",
    "                with Image.open(input_path) as img:\n",
    "                    img_resized = img.resize(target_size, Image.LANCZOS)\n",
    "                    img_resized.save(output_path)\n",
    "    \n",
    "    print(f\"All images have been downsampled and saved to: {output_dir}\")\n",
    "\n",
    "\n",
    "# Cell 3: Example Usage\n",
    "# Replace the following paths with your own directories, then uncomment to run:\n",
    "input_dir = \"test\"\n",
    "output_dir = \"128x128_images\"\n",
    "output_dir = os.path.join(output_dir, input_dir)\n",
    "downsample_images(input_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc529bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from diffusers import UNet2DModel, DDPMScheduler, DDIMScheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "data_dir = \"./128x128_images\"  # Base directory containing class subfolders\n",
    "train_dir = os.path.join(data_dir, \"train\")  # e.g., 128x128_images/train/C0XX\n",
    "val_dir = os.path.join(data_dir, \"validation\")  # e.g., 128x128_images/validation/C0XX\n",
    "batch_size = 32\n",
    "image_size = 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 50\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),\n",
    "])\n",
    "\n",
    "# Datasets and loaders\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Define model and scheduler\n",
    "unet = UNet2DModel(\n",
    "    sample_size=image_size,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(64, 128, 256, 512),\n",
    "    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\"),\n",
    "    up_block_types=(\"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\n",
    ").to(device)\n",
    "\n",
    "# Choose scheduler: DDIM for sampling, but use DDPMScheduler for training noise schedule\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "# For sampling: ddim_scheduler = DDIMScheduler.from_config(noise_scheduler.config)\n",
    "\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "def train():\n",
    "    unet.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for images, _ in loop:\n",
    "            images = images.to(device)\n",
    "            batch_size = images.size(0)\n",
    "\n",
    "            # Sample noise and timesteps\n",
    "            noise = torch.randn_like(images)\n",
    "            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (batch_size,), device=device)\n",
    "\n",
    "            # Add noise to images\n",
    "            noisy_images = noise_scheduler.add_noise(images, noise, timesteps)\n",
    "\n",
    "            # Predict noise\n",
    "            noise_pred = unet(noisy_images, timesteps).sample\n",
    "\n",
    "            # Compute loss\n",
    "            loss = nn.MSELoss()(noise_pred, noise)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Validation and sample generation every few epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            sample_images(epoch + 1)\n",
    "\n",
    "    # Save final model\n",
    "    torch.save(unet.state_dict(), \"ddim_unet.pth\")\n",
    "\n",
    "# Sampling for validation\n",
    "def sample_images(epoch, num_samples=4):\n",
    "    unet.eval()\n",
    "    # ddim scheduler for sampling\n",
    "    ddim_scheduler = DDIMScheduler.from_config(noise_scheduler.config)\n",
    "\n",
    "    # Start from pure noise\n",
    "    latent = torch.randn((num_samples, 3, image_size, image_size), device=device)\n",
    "    for t in tqdm(list(range(ddim_scheduler.config.num_train_timesteps))[::-1], desc=\"Sampling\"):\n",
    "        # Predict noise\n",
    "        with torch.no_grad():\n",
    "            noise_pred = unet(latent, torch.tensor([t]*num_samples, device=device)).sample\n",
    "\n",
    "        # Compute previous sample\n",
    "        latent = ddim_scheduler.step(noise_pred, t, latent).prev_sample\n",
    "\n",
    "    # Denormalize and save\n",
    "    samples = (latent.clamp(-1, 1) + 1) / 2\n",
    "    os.makedirs(\"samples\", exist_ok=True)\n",
    "    for i, img in enumerate(samples):\n",
    "        transforms.ToPILImage()(img.cpu()).save(f\"samples/sample_{epoch}_{i}.png\")\n",
    "    unet.train()\n",
    "\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
