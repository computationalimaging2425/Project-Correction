\subsection{RED-Diff: Regularization by Denoising Diffusion}
\begin{frame}{RED-Diff: Regularization by Denoising Diffusion}
  \begin{itemize}
    \item \textbf{RED-Diff} is an optimization-based method for solving inverse problems using diffusion models
    \item Key idea: combine data fidelity loss with regularization from denoising diffusion priors
    \item Uses gradient-based optimization to reconstruct images by minimizing combined objective
  \end{itemize}
\end{frame}


\begin{frame}{RED-Diff - Overview}
  \textbf{Algorithm Overview:}
  \begin{enumerate}
    \item Initialize reconstruction $\mu$ from adjoint operation: $\mu = K^T(y)$
    \item For each timestep $t$: sample noise and create noisy version $x_t$
    \item Compute data fidelity loss: $\mathcal{L}_{obs} = \frac{1}{2\sigma_y^2}\|K(\mu) - y\|^2$
    \item Compute regularization loss using diffusion model guidance
    \item Update $\mu$ using gradient descent on combined loss
  \end{enumerate}

  \textbf{Combined Objective:}
  $$\mathcal{L} = \mathcal{L}_{obs} + \lambda \cdot w_t \cdot \mathcal{L}_{reg}$$
  where $w_t$ is a time-dependent weighting strategy
\end{frame}

% Frame: Implementation of RED-Diff
\begin{frame}{Implementation of RED-Diff}
  \textbf{Algorithm Steps:}
  \begin{enumerate}
    \item Initialize $\mu \leftarrow K^T(y)$ with gradient enabled
    \item Sample noise $\epsilon \sim \mathcal{N}(0, I)$ and create $x_t = \sqrt{\alpha_t}\mu + \sqrt{1-\alpha_t}\epsilon$
    \item Predict noise $\epsilon_\theta(x_t, t)$ using UNet
    \item Compute regularization loss: $\mathcal{L}_{reg} = w_t \cdot \|\epsilon_\theta - \epsilon\|^2$
    \item Update $\mu$ using Adam optimizer on total loss
    \item Repeat for all timesteps in reverse order
  \end{enumerate}

  \textbf{Weighting Strategies:} linear, sqrt, square, log, clip, const
\end{frame}